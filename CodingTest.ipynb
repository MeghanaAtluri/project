{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e3ee24-c957-4f75-a2e2-44549f3d6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae97a0de-c8e3-452f-8c23-3566191d6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "training= pd.read_excel('data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca697e-16de-4128-894d-ed64f275e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77825ed6-7b2e-48d5-a5c7-496820945415",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words(\"english\")\n",
    "print(f'Final list of stopwords:\\n{stopwords_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588d4ce-c5a9-4fe0-901b-102a346e40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize email\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemma_stem_text(words_list):\n",
    "    # Lemmatizer\n",
    "    text = [lemmatizer.lemmatize(token.lower()) for token in words_list] \n",
    "    text = [lemmatizer.lemmatize(token.lower(), \"v\") for token in text ]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515c199-496e-4900-b786-1b0d57735ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Negative Contractions\n",
    "\n",
    "import re\n",
    "re_negation = re.compile(\"n't \") \n",
    "\n",
    "def negation_abbreviated_to_standard(sent):\n",
    "    sent = re_negation.sub(\" not \", sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee9009-e5b1-4a16-990b-cd00c98bd123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_words(email):\n",
    "    email_text = BeautifulSoup(email).get_text()\n",
    "    \n",
    "    email_text = negation_abbreviated_to_standard(email_text)\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(email_text)\n",
    "\n",
    "    meaningful_words = [w for w in words if w.lower() not in stopwords_list]\n",
    "\n",
    "    lemma_words = lemma_stem_text(meaningful_words)\n",
    "    return( \" \".join(lemma_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef1d69-abb7-47ae-b5b8-82bd6baf151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = training['email']\n",
    "\n",
    "cleaned_train_email = []\n",
    "\n",
    "# We loop over each email and clean it\n",
    "for i in emails:\n",
    "    cleaned_train_email.append(email_to_words(i))\n",
    "\n",
    "print(cleaned_train_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfc2f77-a95f-4036-b6db-43884d9ca33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=20000, ngram_range = (1,2))\n",
    "train_data_features = vectorizer.fit_transform(cleaned_train_email)\n",
    "\n",
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b8677-846d-4ce5-926c-a3f002d42aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_t, X_val_t, Y_train_t, Y_val_t = train_test_split(train_data_features_t, y_train, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03262b2e-a50b-4065-bc13-deb843920aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model_tfidf = SVC(random_state=0,class_weight='balanced')\n",
    "svm_model_tfidf.fit(X_train_t, Y_train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f12238-7278-4bbe-b6e8-686a86d713d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_svm_tfidf = svm_model_tfidf.predict(X_val_t)\n",
    "\n",
    "# Calculate metrics for SVM model\n",
    "accuracy_svm_tfidf = accuracy_score(Y_val_t, y_pred_svm_tfidf)\n",
    "precision_svm_tfidf = precision_score(Y_val_t, y_pred_svm_tfidf)\n",
    "recall_svm_tfidf = recall_score(Y_val_t, y_pred_svm_tfidf)\n",
    "f1_svm_tfidf = f1_score(Y_val_t, y_pred_svm_tfidf)\n",
    "\n",
    "print(\"SVM Model Metrics on TF-IDF encoded data:\")\n",
    "print(\"Accuracy:\", accuracy_svm_tfidf)\n",
    "print(\"Precision:\", precision_svm_tfidf)\n",
    "print(\"Recall:\", recall_svm_tfidf)\n",
    "print(\"F1 Score:\", f1_svm_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
